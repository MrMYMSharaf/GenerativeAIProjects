{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArXive Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query to fetch AI-releted papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ai or LLM Or artificial intelligence or machine Learning\"\n",
    "search = arxiv.Search(query=query,max_results = 10, sort_by = arxiv.SortCriterion.SubmittedDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_12804\\3825748363.py:2: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    }
   ],
   "source": [
    "papers = []\n",
    "for result in search.results():\n",
    "  papers.append({\n",
    "    \"published\":result.published,\n",
    "    \"title\":result.title,\n",
    "    \"abstract\":result.summary,\n",
    "    \"categories\":result.categories,\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-12-06 18:59:56+00:00</td>\n",
       "      <td>Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model</td>\n",
       "      <td>4D driving simulation is essential for developing realistic autonomous\\ndriving simulators. Despite advancements in existing methods for generating\\ndriving scenes, significant challenges remain in view transformation and\\nspatial-temporal dynamic modeling. To address these limitations, we propose a\\nSpatial-Temporal simulAtion for drivinG (Stag-1) model to reconstruct\\nreal-world scenes and design a controllable generative network to achieve 4D\\nsimulation. Stag-1 constructs continuous 4D point cloud scenes using\\nsurround-view data from autonomous vehicles. It decouples spatial-temporal\\nrelationships and produces coherent keyframe videos. Additionally, Stag-1\\nleverages video generation models to obtain photo-realistic and controllable 4D\\ndriving simulation videos from any perspective. To expand the range of view\\ngeneration, we train vehicle motion videos based on decomposed camera poses,\\nenhancing modeling capabilities for distant scenes. Furthermore, we reconstruct\\nvehicle camera trajectories to integrate 3D points across consecutive views,\\nenabling comprehensive scene understanding along the temporal dimension.\\nFollowing extensive multi-level scene training, Stag-1 can simulate from any\\ndesired viewpoint and achieve a deep understanding of scene evolution under\\nstatic spatial-temporal conditions. Compared to existing methods, our approach\\nshows promising performance in multi-view scene consistency, background\\ncoherence, and accuracy, and contributes to the ongoing advancements in\\nrealistic autonomous driving simulation. Code: https://github.com/wzzheng/Stag.</td>\n",
       "      <td>[cs.CV, cs.AI, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12-06 18:59:53+00:00</td>\n",
       "      <td>Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories</td>\n",
       "      <td>The fields of 3D reconstruction and text-based 3D editing have advanced\\nsignificantly with the evolution of text-based diffusion models. While existing\\n3D editing methods excel at modifying color, texture, and style, they struggle\\nwith extensive geometric or appearance changes, thus limiting their\\napplications. We propose Perturb-and-Revise, which makes possible a variety of\\nNeRF editing. First, we perturb the NeRF parameters with random initializations\\nto create a versatile initialization. We automatically determine the\\nperturbation magnitude through analysis of the local loss landscape. Then, we\\nrevise the edited NeRF via generative trajectories. Combined with the\\ngenerative process, we impose identity-preserving gradients to refine the\\nedited NeRF. Extensive experiments demonstrate that Perturb-and-Revise\\nfacilitates flexible, effective, and consistent editing of color, appearance,\\nand geometry in 3D. For 360{\\deg} results, please visit our project page:\\nhttps://susunghong.github.io/Perturb-and-Revise.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-12-06 18:59:52+00:00</td>\n",
       "      <td>Birth and Death of a Rose</td>\n",
       "      <td>We study the problem of generating temporal object intrinsics -- temporally\\nevolving sequences of object geometry, reflectance, and texture, such as a\\nblooming rose -- from pre-trained 2D foundation models. Unlike conventional 3D\\nmodeling and animation techniques that require extensive manual effort and\\nexpertise, we introduce a method that generates such assets with signals\\ndistilled from pre-trained 2D diffusion models. To ensure the temporal\\nconsistency of object intrinsics, we propose Neural Templates for\\ntemporal-state-guided distillation, derived automatically from image features\\nfrom self-supervised learning. Our method can generate high-quality temporal\\nobject intrinsics for several natural phenomena and enable the sampling and\\ncontrollable rendering of these dynamic objects from any viewpoint, under any\\nenvironmental lighting conditions, at any time of their lifespan. Project\\nwebsite: https://chen-geng.com/rose4d</td>\n",
       "      <td>[cs.CV, cs.GR, I.2.10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-12-06 18:59:51+00:00</td>\n",
       "      <td>Sparse autoencoders reveal selective remapping of visual concepts during adaptation</td>\n",
       "      <td>Adapting foundation models for specific purposes has become a standard\\napproach to build machine learning systems for downstream applications. Yet, it\\nis an open question which mechanisms take place during adaptation. Here we\\ndevelop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named\\nPatchSAE, to extract interpretable concepts at granular levels (e.g. shape,\\ncolor, or semantics of an object) and their patch-wise spatial attributions. We\\nexplore how these concepts influence the model output in downstream image\\nclassification tasks and investigate how recent state-of-the-art prompt-based\\nadaptation techniques change the association of model inputs to these concepts.\\nWhile activations of concepts slightly change between adapted and non-adapted\\nmodels, we find that the majority of gains on common adaptation tasks can be\\nexplained with the existing concepts already present in the non-adapted\\nfoundation model. This work provides a concrete framework to train and use SAEs\\nfor Vision Transformers and provides insights into explaining adaptation\\nmechanisms.</td>\n",
       "      <td>[cs.CV, cs.LG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-12-06 18:59:51+00:00</td>\n",
       "      <td>Text to Blind Motion</td>\n",
       "      <td>People who are blind perceive the world differently than those who are\\nsighted, which can result in distinct motion characteristics. For instance,\\nwhen crossing at an intersection, blind individuals may have different patterns\\nof movement, such as veering more from a straight path or using touch-based\\nexploration around curbs and obstacles. These behaviors may appear less\\npredictable to motion models embedded in technologies such as autonomous\\nvehicles. Yet, the ability of 3D motion models to capture such behavior has not\\nbeen previously studied, as existing datasets for 3D human motion currently\\nlack diversity and are biased toward people who are sighted. In this work, we\\nintroduce BlindWays, the first multimodal motion benchmark for pedestrians who\\nare blind. We collect 3D motion data using wearable sensors with 11 blind\\nparticipants navigating eight different routes in a real-world urban setting.\\nAdditionally, we provide rich textual descriptions that capture the distinctive\\nmovement characteristics of blind pedestrians and their interactions with both\\nthe navigation aid (e.g., a white cane or a guide dog) and the environment. We\\nbenchmark state-of-the-art 3D human prediction models, finding poor performance\\nwith off-the-shelf and pre-training-based methods for our novel task. To\\ncontribute toward safer and more reliable systems that can seamlessly reason\\nover diverse human movements in their environments, our text-and-motion\\nbenchmark is available at https://blindways.github.io.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-12-06 18:59:12+00:00</td>\n",
       "      <td>MotionFlow: Attention-Driven Motion Transfer in Video Diffusion Models</td>\n",
       "      <td>Text-to-video models have demonstrated impressive capabilities in producing\\ndiverse and captivating video content, showcasing a notable advancement in\\ngenerative AI. However, these models generally lack fine-grained control over\\nmotion patterns, limiting their practical applicability. We introduce\\nMotionFlow, a novel framework designed for motion transfer in video diffusion\\nmodels. Our method utilizes cross-attention maps to accurately capture and\\nmanipulate spatial and temporal dynamics, enabling seamless motion transfers\\nacross various contexts. Our approach does not require training and works on\\ntest-time by leveraging the inherent capabilities of pre-trained video\\ndiffusion models. In contrast to traditional approaches, which struggle with\\ncomprehensive scene changes while maintaining consistent motion, MotionFlow\\nsuccessfully handles such complex transformations through its attention-based\\nmechanism. Our qualitative and quantitative experiments demonstrate that\\nMotionFlow significantly outperforms existing models in both fidelity and\\nversatility even during drastic scene alterations.</td>\n",
       "      <td>[cs.CV, cs.AI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-12-06 18:59:04+00:00</td>\n",
       "      <td>SimC3D: A Simple Contrastive 3D Pretraining Framework Using RGB Images</td>\n",
       "      <td>The 3D contrastive learning paradigm has demonstrated remarkable performance\\nin downstream tasks through pretraining on point cloud data. Recent advances\\ninvolve additional 2D image priors associated with 3D point clouds for further\\nimprovement. Nonetheless, these existing frameworks are constrained by the\\nrestricted range of available point cloud datasets, primarily due to the high\\ncosts of obtaining point cloud data. To this end, we propose SimC3D, a simple\\nbut effective 3D contrastive learning framework, for the first time,\\npretraining 3D backbones from pure RGB image data. SimC3D performs contrastive\\n3D pretraining with three appealing properties. (1) Pure image data: SimC3D\\nsimplifies the dependency of costly 3D point clouds and pretrains 3D backbones\\nusing solely RBG images. By employing depth estimation and suitable data\\nprocessing, the monocular synthesized point cloud shows great potential for 3D\\npretraining. (2) Simple framework: Traditional multi-modal frameworks\\nfacilitate 3D pretraining with 2D priors by utilizing an additional 2D\\nbackbone, thereby increasing computational expense. In this paper, we\\nempirically demonstrate that the primary benefit of the 2D modality stems from\\nthe incorporation of locality information. Inspired by this insightful\\nobservation, SimC3D directly employs 2D positional embeddings as a stronger\\ncontrastive objective, eliminating the necessity for 2D backbones and leading\\nto considerable performance improvements. (3) Strong performance: SimC3D\\noutperforms previous approaches that leverage ground-truth point cloud data for\\npretraining in various downstream tasks. Furthermore, the performance of SimC3D\\ncan be further enhanced by combining multiple image datasets, showcasing its\\nsignificant potential for scalability. The code will be available at\\nhttps://github.com/Dongjiahua/SimC3D.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-12-06 18:57:40+00:00</td>\n",
       "      <td>Real-space chirality from crystalline topological defects in the Kitaev spin liquid</td>\n",
       "      <td>We show that certain crystalline topological defects in the gapless Kitaev\\nhoneycomb spin liquid model generate a chirality that depends in a universal\\nmanner on their emergent flux. Focusing on 5-7 dislocations as building blocks,\\nconsisting of pentagon and heptagon disclinations, we identify the Kitaev bond\\nlabel configurations that preserve solvability. By defining and computing\\nmultiple formulations of local Chern markers we find that the 5 and 7 lattice\\ndefects host a contribution to real-space chirality, which we dub \"Chern\\ncharge\" or $\\mathcal{Q_M}$. The sign of this local Chern charge obeys\\n$\\text{sgn}({\\mathcal{Q_M}}) = - i ~ \\text{sgn}(F) W$ where $F$ is the\\ndisclination Frank angle and $W = \\pm i$ is its emergent-gauge-field flux. We\\nshow that lattice curvature and torsion can interplay with the surrounding\\ngapless background to modify the $\\mathcal{Q_M}$ profile and magnitude, but\\nthat the sign of $\\mathcal{Q_M}$ is set by local properties of the defect,\\nenabling the identification of Chern charges.</td>\n",
       "      <td>[cond-mat.str-el, cond-mat.dis-nn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-12-06 18:57:08+00:00</td>\n",
       "      <td>Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling</td>\n",
       "      <td>We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)\\nseries that builds upon InternVL 2.0, maintaining its core model architecture\\nwhile introducing significant enhancements in training and testing strategies\\nas well as data quality. In this work, we delve into the relationship between\\nmodel scaling and performance, systematically exploring the performance trends\\nin vision encoders, language models, dataset sizes, and test-time\\nconfigurations. Through extensive evaluations on a wide range of benchmarks,\\nincluding multi-discipline reasoning, document understanding, multi-image /\\nvideo understanding, real-world comprehension, multimodal hallucination\\ndetection, visual grounding, multilingual capabilities, and pure language\\nprocessing, InternVL 2.5 exhibits competitive performance, rivaling leading\\ncommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is\\nthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a\\n3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing\\nstrong potential for test-time scaling. We hope this model contributes to the\\nopen-source community by setting new standards for developing and applying\\nmultimodal AI systems. HuggingFace demo see\\nhttps://huggingface.co/spaces/OpenGVLab/InternVL</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-12-06 18:55:34+00:00</td>\n",
       "      <td>APOLLO: SGD-like Memory, AdamW-level Performance</td>\n",
       "      <td>Large language models (LLMs) are notoriously memory-intensive during\\ntraining, particularly with the popular AdamW optimizer. This memory burden\\nnecessitates using more or higher-end GPUs or reducing batch sizes, limiting\\ntraining scalability and throughput. To address this, various memory-efficient\\noptimizers have been proposed to reduce optimizer memory usage. However, they\\nface critical challenges: (i) reliance on costly SVD operations; (ii)\\nsignificant performance trade-offs compared to AdamW; and (iii) still\\nsubstantial optimizer memory overhead to maintain competitive performance.\\n  In this work, we identify that AdamW's learning rate adaptation rule can be\\neffectively coarsened as a structured learning rate update. Based on this\\ninsight, we propose Approximated Gradient Scaling for Memory-Efficient LLM\\nOptimization (APOLLO), which approximates learning rate scaling using an\\nauxiliary low-rank optimizer state based on pure random projection. This\\nstructured learning rate update rule makes APOLLO highly tolerant to further\\nmemory reductions while delivering comparable pre-training performance. Even\\nits rank-1 variant, APOLLO-Mini, achieves superior pre-training performance\\ncompared to AdamW with SGD-level memory costs.\\n  Extensive experiments demonstrate that the APOLLO series performs on-par with\\nor better than AdamW, while achieving greater memory savings by nearly\\neliminating the optimization states of AdamW. These savings provide significant\\nsystem-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB\\nsetup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model\\nScalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without\\nsystem-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training\\nLLaMA-7B on a single GPU using less than 12 GB of memory with weight\\nquantization.</td>\n",
       "      <td>[cs.LG, cs.AI, cs.PF]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  published  \\\n",
       "0 2024-12-06 18:59:56+00:00   \n",
       "1 2024-12-06 18:59:53+00:00   \n",
       "2 2024-12-06 18:59:52+00:00   \n",
       "3 2024-12-06 18:59:51+00:00   \n",
       "4 2024-12-06 18:59:51+00:00   \n",
       "5 2024-12-06 18:59:12+00:00   \n",
       "6 2024-12-06 18:59:04+00:00   \n",
       "7 2024-12-06 18:57:40+00:00   \n",
       "8 2024-12-06 18:57:08+00:00   \n",
       "9 2024-12-06 18:55:34+00:00   \n",
       "\n",
       "                                                                                                       title  \\\n",
       "0                                Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model   \n",
       "1                                       Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories   \n",
       "2                                                                                  Birth and Death of a Rose   \n",
       "3                        Sparse autoencoders reveal selective remapping of visual concepts during adaptation   \n",
       "4                                                                                       Text to Blind Motion   \n",
       "5                                     MotionFlow: Attention-Driven Motion Transfer in Video Diffusion Models   \n",
       "6                                     SimC3D: A Simple Contrastive 3D Pretraining Framework Using RGB Images   \n",
       "7                        Real-space chirality from crystalline topological defects in the Kitaev spin liquid   \n",
       "8  Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling   \n",
       "9                                                           APOLLO: SGD-like Memory, AdamW-level Performance   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      abstract  \\\n",
       "0                                                                                                                                                                                                                                                                                                            4D driving simulation is essential for developing realistic autonomous\\ndriving simulators. Despite advancements in existing methods for generating\\ndriving scenes, significant challenges remain in view transformation and\\nspatial-temporal dynamic modeling. To address these limitations, we propose a\\nSpatial-Temporal simulAtion for drivinG (Stag-1) model to reconstruct\\nreal-world scenes and design a controllable generative network to achieve 4D\\nsimulation. Stag-1 constructs continuous 4D point cloud scenes using\\nsurround-view data from autonomous vehicles. It decouples spatial-temporal\\nrelationships and produces coherent keyframe videos. Additionally, Stag-1\\nleverages video generation models to obtain photo-realistic and controllable 4D\\ndriving simulation videos from any perspective. To expand the range of view\\ngeneration, we train vehicle motion videos based on decomposed camera poses,\\nenhancing modeling capabilities for distant scenes. Furthermore, we reconstruct\\nvehicle camera trajectories to integrate 3D points across consecutive views,\\nenabling comprehensive scene understanding along the temporal dimension.\\nFollowing extensive multi-level scene training, Stag-1 can simulate from any\\ndesired viewpoint and achieve a deep understanding of scene evolution under\\nstatic spatial-temporal conditions. Compared to existing methods, our approach\\nshows promising performance in multi-view scene consistency, background\\ncoherence, and accuracy, and contributes to the ongoing advancements in\\nrealistic autonomous driving simulation. Code: https://github.com/wzzheng/Stag.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The fields of 3D reconstruction and text-based 3D editing have advanced\\nsignificantly with the evolution of text-based diffusion models. While existing\\n3D editing methods excel at modifying color, texture, and style, they struggle\\nwith extensive geometric or appearance changes, thus limiting their\\napplications. We propose Perturb-and-Revise, which makes possible a variety of\\nNeRF editing. First, we perturb the NeRF parameters with random initializations\\nto create a versatile initialization. We automatically determine the\\nperturbation magnitude through analysis of the local loss landscape. Then, we\\nrevise the edited NeRF via generative trajectories. Combined with the\\ngenerative process, we impose identity-preserving gradients to refine the\\nedited NeRF. Extensive experiments demonstrate that Perturb-and-Revise\\nfacilitates flexible, effective, and consistent editing of color, appearance,\\nand geometry in 3D. For 360{\\deg} results, please visit our project page:\\nhttps://susunghong.github.io/Perturb-and-Revise.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          We study the problem of generating temporal object intrinsics -- temporally\\nevolving sequences of object geometry, reflectance, and texture, such as a\\nblooming rose -- from pre-trained 2D foundation models. Unlike conventional 3D\\nmodeling and animation techniques that require extensive manual effort and\\nexpertise, we introduce a method that generates such assets with signals\\ndistilled from pre-trained 2D diffusion models. To ensure the temporal\\nconsistency of object intrinsics, we propose Neural Templates for\\ntemporal-state-guided distillation, derived automatically from image features\\nfrom self-supervised learning. Our method can generate high-quality temporal\\nobject intrinsics for several natural phenomena and enable the sampling and\\ncontrollable rendering of these dynamic objects from any viewpoint, under any\\nenvironmental lighting conditions, at any time of their lifespan. Project\\nwebsite: https://chen-geng.com/rose4d   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Adapting foundation models for specific purposes has become a standard\\napproach to build machine learning systems for downstream applications. Yet, it\\nis an open question which mechanisms take place during adaptation. Here we\\ndevelop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named\\nPatchSAE, to extract interpretable concepts at granular levels (e.g. shape,\\ncolor, or semantics of an object) and their patch-wise spatial attributions. We\\nexplore how these concepts influence the model output in downstream image\\nclassification tasks and investigate how recent state-of-the-art prompt-based\\nadaptation techniques change the association of model inputs to these concepts.\\nWhile activations of concepts slightly change between adapted and non-adapted\\nmodels, we find that the majority of gains on common adaptation tasks can be\\nexplained with the existing concepts already present in the non-adapted\\nfoundation model. This work provides a concrete framework to train and use SAEs\\nfor Vision Transformers and provides insights into explaining adaptation\\nmechanisms.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                       People who are blind perceive the world differently than those who are\\nsighted, which can result in distinct motion characteristics. For instance,\\nwhen crossing at an intersection, blind individuals may have different patterns\\nof movement, such as veering more from a straight path or using touch-based\\nexploration around curbs and obstacles. These behaviors may appear less\\npredictable to motion models embedded in technologies such as autonomous\\nvehicles. Yet, the ability of 3D motion models to capture such behavior has not\\nbeen previously studied, as existing datasets for 3D human motion currently\\nlack diversity and are biased toward people who are sighted. In this work, we\\nintroduce BlindWays, the first multimodal motion benchmark for pedestrians who\\nare blind. We collect 3D motion data using wearable sensors with 11 blind\\nparticipants navigating eight different routes in a real-world urban setting.\\nAdditionally, we provide rich textual descriptions that capture the distinctive\\nmovement characteristics of blind pedestrians and their interactions with both\\nthe navigation aid (e.g., a white cane or a guide dog) and the environment. We\\nbenchmark state-of-the-art 3D human prediction models, finding poor performance\\nwith off-the-shelf and pre-training-based methods for our novel task. To\\ncontribute toward safer and more reliable systems that can seamlessly reason\\nover diverse human movements in their environments, our text-and-motion\\nbenchmark is available at https://blindways.github.io.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Text-to-video models have demonstrated impressive capabilities in producing\\ndiverse and captivating video content, showcasing a notable advancement in\\ngenerative AI. However, these models generally lack fine-grained control over\\nmotion patterns, limiting their practical applicability. We introduce\\nMotionFlow, a novel framework designed for motion transfer in video diffusion\\nmodels. Our method utilizes cross-attention maps to accurately capture and\\nmanipulate spatial and temporal dynamics, enabling seamless motion transfers\\nacross various contexts. Our approach does not require training and works on\\ntest-time by leveraging the inherent capabilities of pre-trained video\\ndiffusion models. In contrast to traditional approaches, which struggle with\\ncomprehensive scene changes while maintaining consistent motion, MotionFlow\\nsuccessfully handles such complex transformations through its attention-based\\nmechanism. Our qualitative and quantitative experiments demonstrate that\\nMotionFlow significantly outperforms existing models in both fidelity and\\nversatility even during drastic scene alterations.   \n",
       "6                           The 3D contrastive learning paradigm has demonstrated remarkable performance\\nin downstream tasks through pretraining on point cloud data. Recent advances\\ninvolve additional 2D image priors associated with 3D point clouds for further\\nimprovement. Nonetheless, these existing frameworks are constrained by the\\nrestricted range of available point cloud datasets, primarily due to the high\\ncosts of obtaining point cloud data. To this end, we propose SimC3D, a simple\\nbut effective 3D contrastive learning framework, for the first time,\\npretraining 3D backbones from pure RGB image data. SimC3D performs contrastive\\n3D pretraining with three appealing properties. (1) Pure image data: SimC3D\\nsimplifies the dependency of costly 3D point clouds and pretrains 3D backbones\\nusing solely RBG images. By employing depth estimation and suitable data\\nprocessing, the monocular synthesized point cloud shows great potential for 3D\\npretraining. (2) Simple framework: Traditional multi-modal frameworks\\nfacilitate 3D pretraining with 2D priors by utilizing an additional 2D\\nbackbone, thereby increasing computational expense. In this paper, we\\nempirically demonstrate that the primary benefit of the 2D modality stems from\\nthe incorporation of locality information. Inspired by this insightful\\nobservation, SimC3D directly employs 2D positional embeddings as a stronger\\ncontrastive objective, eliminating the necessity for 2D backbones and leading\\nto considerable performance improvements. (3) Strong performance: SimC3D\\noutperforms previous approaches that leverage ground-truth point cloud data for\\npretraining in various downstream tasks. Furthermore, the performance of SimC3D\\ncan be further enhanced by combining multiple image datasets, showcasing its\\nsignificant potential for scalability. The code will be available at\\nhttps://github.com/Dongjiahua/SimC3D.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              We show that certain crystalline topological defects in the gapless Kitaev\\nhoneycomb spin liquid model generate a chirality that depends in a universal\\nmanner on their emergent flux. Focusing on 5-7 dislocations as building blocks,\\nconsisting of pentagon and heptagon disclinations, we identify the Kitaev bond\\nlabel configurations that preserve solvability. By defining and computing\\nmultiple formulations of local Chern markers we find that the 5 and 7 lattice\\ndefects host a contribution to real-space chirality, which we dub \"Chern\\ncharge\" or $\\mathcal{Q_M}$. The sign of this local Chern charge obeys\\n$\\text{sgn}({\\mathcal{Q_M}}) = - i ~ \\text{sgn}(F) W$ where $F$ is the\\ndisclination Frank angle and $W = \\pm i$ is its emergent-gauge-field flux. We\\nshow that lattice curvature and torsion can interplay with the surrounding\\ngapless background to modify the $\\mathcal{Q_M}$ profile and magnitude, but\\nthat the sign of $\\mathcal{Q_M}$ is set by local properties of the defect,\\nenabling the identification of Chern charges.   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             We introduce InternVL 2.5, an advanced multimodal large language model (MLLM)\\nseries that builds upon InternVL 2.0, maintaining its core model architecture\\nwhile introducing significant enhancements in training and testing strategies\\nas well as data quality. In this work, we delve into the relationship between\\nmodel scaling and performance, systematically exploring the performance trends\\nin vision encoders, language models, dataset sizes, and test-time\\nconfigurations. Through extensive evaluations on a wide range of benchmarks,\\nincluding multi-discipline reasoning, document understanding, multi-image /\\nvideo understanding, real-world comprehension, multimodal hallucination\\ndetection, visual grounding, multilingual capabilities, and pure language\\nprocessing, InternVL 2.5 exhibits competitive performance, rivaling leading\\ncommercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is\\nthe first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a\\n3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing\\nstrong potential for test-time scaling. We hope this model contributes to the\\nopen-source community by setting new standards for developing and applying\\nmultimodal AI systems. HuggingFace demo see\\nhttps://huggingface.co/spaces/OpenGVLab/InternVL   \n",
       "9  Large language models (LLMs) are notoriously memory-intensive during\\ntraining, particularly with the popular AdamW optimizer. This memory burden\\nnecessitates using more or higher-end GPUs or reducing batch sizes, limiting\\ntraining scalability and throughput. To address this, various memory-efficient\\noptimizers have been proposed to reduce optimizer memory usage. However, they\\nface critical challenges: (i) reliance on costly SVD operations; (ii)\\nsignificant performance trade-offs compared to AdamW; and (iii) still\\nsubstantial optimizer memory overhead to maintain competitive performance.\\n  In this work, we identify that AdamW's learning rate adaptation rule can be\\neffectively coarsened as a structured learning rate update. Based on this\\ninsight, we propose Approximated Gradient Scaling for Memory-Efficient LLM\\nOptimization (APOLLO), which approximates learning rate scaling using an\\nauxiliary low-rank optimizer state based on pure random projection. This\\nstructured learning rate update rule makes APOLLO highly tolerant to further\\nmemory reductions while delivering comparable pre-training performance. Even\\nits rank-1 variant, APOLLO-Mini, achieves superior pre-training performance\\ncompared to AdamW with SGD-level memory costs.\\n  Extensive experiments demonstrate that the APOLLO series performs on-par with\\nor better than AdamW, while achieving greater memory savings by nearly\\neliminating the optimization states of AdamW. These savings provide significant\\nsystem-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB\\nsetup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model\\nScalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without\\nsystem-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training\\nLLaMA-7B on a single GPU using less than 12 GB of memory with weight\\nquantization.   \n",
       "\n",
       "                           categories  \n",
       "0               [cs.CV, cs.AI, cs.LG]  \n",
       "1                             [cs.CV]  \n",
       "2              [cs.CV, cs.GR, I.2.10]  \n",
       "3                      [cs.CV, cs.LG]  \n",
       "4                             [cs.CV]  \n",
       "5                      [cs.CV, cs.AI]  \n",
       "6                             [cs.CV]  \n",
       "7  [cond-mat.str-el, cond-mat.dis-nn]  \n",
       "8                             [cs.CV]  \n",
       "9               [cs.LG, cs.AI, cs.PF]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(papers)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1581"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['abstract'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "abstract = df['abstract'][0]\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000\n",
    "# Split the abstract into chunks\n",
    "chunks = [abstract[i:i + chunk_size] for i in range(0, len(abstract), chunk_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_chunks = []\n",
    "for chunk in chunks:\n",
    "    summary = summarizer(chunk, max_length=116, min_length=10, do_sample=False)\n",
    "    summarized_chunks.append(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drivinG (Stag-1) model reconstructs real-world scenes . it decouples spatial-temporal relationships and produces coherent keyframe videos . to expand the range of view generation, we train vehicle motion videos based on decomposed camera poses . ies to integrate 3D points across consecutive views . enables comprehensive scene understanding along the temporal dimension .\n"
     ]
    }
   ],
   "source": [
    "# Join the summaries of the chunks together\n",
    "final_summary = \" \".join(summarized_chunks)\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1581, but your input_length is only 313. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=156)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stag-1 constructs continuous 4D point cloud scenes using surround-view data . it decouples spatial-temporal relationships and produces coherent keyframe videos . to expand the range of view generation, we train vehicle motion videos based on decomposed camera poses .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Summarization\n",
    "summarization_result = summarizer(abstract, max_length=1581, min_length=10, do_sample=False)\n",
    "print(summarization_result[0]['summary_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
